{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Activity: Comparing Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this extension activity, you'll load a different, more powerful language model (Qwen3-8B) and compare its performance to the TinyLlama model you've been using. This will help you understand how model size and training affect AI capabilities.\n",
    "\n",
    "**What you'll explore:**\n",
    "- How to load and compare different language models\n",
    "- The relationship between model size and capability\n",
    "- Tradeoffs between speed and accuracy\n",
    "- How to choose the right model for different applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 11 - Understanding Model Differences\n",
    "\n",
    "**TinyLlama-1.1B:**\n",
    "- Size: 1.1 billion parameters\n",
    "- Strengths: Fast, runs on most computers, good for learning\n",
    "- Limitations: Limited reasoning ability, shorter context understanding\n",
    "\n",
    "**Qwen2.5-8B:**\n",
    "- Size: 8 billion parameters (over 7x larger!)\n",
    "- Strengths: Better reasoning, more accurate, better instruction following\n",
    "- Limitations: Requires more memory and processing power, slower responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 12 - Setting Up Both Models\n",
    "\n",
    "We'll set up both TinyLlama and Qwen3-8B from scratch so we can compare them side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.0 - Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core LLM libraries\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Transformers for loading models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.1 - Load TinyLlama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TinyLlama - the smaller, faster model\n",
    "print(\"üì• Loading TinyLlama-1.1B model...\")\n",
    "print(\"‚è≥ This may take a few minutes on first run...\")\n",
    "\n",
    "tiny_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tiny_tokenizer = AutoTokenizer.from_pretrained(tiny_model_name)\n",
    "tiny_model = AutoModelForCausalLM.from_pretrained(\n",
    "    tiny_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "tiny_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=tiny_model,\n",
    "    tokenizer=tiny_tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Wrap for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=tiny_pipe)\n",
    "\n",
    "print(\"‚úÖ TinyLlama model loaded successfully!\")\n",
    "print(f\"üìä Model size: ~1.1 billion parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.2 - Load the Qwen3-8B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Qwen3-8B model - the larger, more powerful model\n",
    "print(\"üì• Loading Qwen2.5-8B model...\")\n",
    "print(\"‚è≥ This is a larger model and will take longer to load...\")\n",
    "\n",
    "qwen_model_name = \"Qwen/Qwen2.5-7B-Instruct\"  # Using 7B variant\n",
    "\n",
    "# Load tokenizer and model\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_model_name)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "qwen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=qwen_model,\n",
    "    tokenizer=qwen_tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Wrap for LangChain\n",
    "qwen_llm = HuggingFacePipeline(pipeline=qwen_pipe)\n",
    "\n",
    "print(\"‚úÖ Qwen3 model loaded successfully!\")\n",
    "print(f\"üìä Model size: ~7-8 billion parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 45\n",
    "How long did it take to load each model? Which one took longer and why do you think that is?\n",
    "- **TinyLlama load time:**\n",
    "- **Qwen load time:**\n",
    "- **Explanation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 13 - Direct Comparison Tests\n",
    "\n",
    "Let's test both models with the same prompts to see how they differ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.0 - Compare Basic Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison test prompts\n",
    "comparison_prompts = [\n",
    "    \"Explain what machine learning is in one sentence.\",\n",
    "    \"Write a creative story starter (2-3 sentences) about a robot learning to paint.\",\n",
    "    \"List 3 pros and cons of social media.\",\n",
    "    \"Solve this problem: If a train travels 60 miles in 45 minutes, what is its speed in miles per hour?\"\n",
    "]\n",
    "\n",
    "print(\"üî¨ COMPARISON TEST: TinyLlama vs Qwen3\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, prompt in enumerate(comparison_prompts, 1):\n",
    "    print(f\"\\nüìù Test {i}: {prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # TinyLlama response\n",
    "    print(\"\\nü§ñ TinyLlama Response:\")\n",
    "    tiny_response = llm.invoke(prompt)\n",
    "    print(tiny_response)\n",
    "    \n",
    "    # Qwen response\n",
    "    print(\"\\nü§ñ Qwen3 Response:\")\n",
    "    qwen_response = qwen_llm.invoke(prompt)\n",
    "    print(qwen_response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 46\n",
    "Which model gave more accurate responses? Give specific examples.\n",
    "- **Answer:**\n",
    "\n",
    "##### Question 47\n",
    "Which model gave more detailed or elaborate responses?\n",
    "- **Answer:**\n",
    "\n",
    "##### Question 48\n",
    "Did you notice any difference in the writing style or tone between the two models?\n",
    "- **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 14 - Custom AI Assistant Comparison\n",
    "\n",
    "Now let's test your custom AI assistant from the main lab with both models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14.0 - Create Two Versions of Your Custom AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy your custom system prompt from Part 6 or Part 8 of the main lab\n",
    "my_comparison_prompt = \"\"\"\n",
    "[PASTE YOUR SYSTEM PROMPT HERE]\n",
    "\"\"\"\n",
    "\n",
    "# Create template (same for both)\n",
    "comparison_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", my_comparison_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create two chains - one for each model\n",
    "tiny_custom_chain = comparison_template | llm        # TinyLlama version\n",
    "qwen_custom_chain = comparison_template | qwen_llm   # Qwen version\n",
    "\n",
    "print(\"‚úÖ Created two versions of your custom AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14.1 - Test Both Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write 3 test questions for your custom AI\n",
    "my_comparison_questions = [\n",
    "    \"Question 1\",\n",
    "    \"Question 2\",\n",
    "    \"Question 3\"\n",
    "]\n",
    "\n",
    "print(\"üî¨ Testing Your Custom AI with Both Models\\n\")\n",
    "\n",
    "for question in my_comparison_questions:\n",
    "    print(f\"üìù Question: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nü§ñ TinyLlama Version:\")\n",
    "    tiny_response = tiny_custom_chain.invoke({\"question\": question})\n",
    "    print(tiny_response)\n",
    "    \n",
    "    print(\"\\nü§ñ Qwen3 Version:\")\n",
    "    qwen_response = qwen_custom_chain.invoke({\"question\": question})\n",
    "    print(qwen_response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 49\n",
    "Did both models follow your system prompt instructions equally well? Which one stayed more \"in character\"?\n",
    "- **Answer:**\n",
    "\n",
    "##### Question 50\n",
    "Rate each model's performance on your custom AI task (1-10):\n",
    "- TinyLlama: _____ / 10\n",
    "- Qwen3: _____ / 10\n",
    "- Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 15 - Response Speed Test\n",
    "\n",
    "Let's measure how fast each model generates responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.0 - Compare Generation Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_prompt = \"Explain why the sky is blue in simple terms.\"\n",
    "\n",
    "# Time TinyLlama\n",
    "print(\"‚è±Ô∏è  Testing TinyLlama speed...\")\n",
    "start_time = time.time()\n",
    "tiny_response = llm.invoke(test_prompt)\n",
    "tiny_time = time.time() - start_time\n",
    "\n",
    "# Time Qwen\n",
    "print(\"‚è±Ô∏è  Testing Qwen3 speed...\")\n",
    "start_time = time.time()\n",
    "qwen_response = qwen_llm.invoke(test_prompt)\n",
    "qwen_time = time.time() - start_time\n",
    "\n",
    "# Compare\n",
    "print(\"\\nüìä SPEED COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"TinyLlama: {tiny_time:.2f} seconds\")\n",
    "print(f\"Qwen3:     {qwen_time:.2f} seconds\")\n",
    "print(f\"Difference: {abs(tiny_time - qwen_time):.2f} seconds\")\n",
    "\n",
    "if tiny_time < qwen_time:\n",
    "    print(f\"\\n‚úÖ TinyLlama was {(qwen_time/tiny_time):.1f}x faster\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Qwen3 was {(tiny_time/qwen_time):.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 51\n",
    "Which model was faster? By how much?\n",
    "- **Answer:**\n",
    "\n",
    "##### Question 52\n",
    "Based on the speed difference, when might you choose to use the smaller model vs. the larger model?\n",
    "- **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 16 - Complex Reasoning Test\n",
    "\n",
    "Let's test how well each model can handle logical reasoning and problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16.0 - Test Logical Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex reasoning prompts\n",
    "reasoning_prompts = [\n",
    "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain your reasoning.\",\n",
    "    \"A farmer has 17 sheep. All but 9 die. How many sheep are left?\",\n",
    "    \"What comes next in this pattern: 2, 6, 12, 20, 30, ___?\"\n",
    "]\n",
    "\n",
    "print(\"üß† REASONING TEST\\n\")\n",
    "\n",
    "for prompt in reasoning_prompts:\n",
    "    print(f\"üìù Challenge: {prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nü§ñ TinyLlama:\")\n",
    "    tiny_response = llm.invoke(prompt)\n",
    "    print(tiny_response)\n",
    "    \n",
    "    print(\"\\nü§ñ Qwen3:\")\n",
    "    qwen_response = qwen_llm.invoke(prompt)\n",
    "    print(qwen_response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 53\n",
    "Which model performed better on logical reasoning tasks? Give specific examples.\n",
    "- **Answer:**\n",
    "\n",
    "##### Question 54\n",
    "Did either model make mistakes? What kind of errors did you notice?\n",
    "- **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 17 - Final Analysis\n",
    "\n",
    "Now that you've tested both models extensively, let's analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 55\n",
    "Create a comparison table:\n",
    "\n",
    "| Feature | TinyLlama (1.1B) | Qwen3 (7-8B) | Winner |\n",
    "|---------|------------------|--------------|--------|\n",
    "| Speed | | | |\n",
    "| Accuracy | | | |\n",
    "| Instruction Following | | | |\n",
    "| Creativity | | | |\n",
    "| Reasoning Ability | | | |\n",
    "| Resource Usage | | | |\n",
    "\n",
    "##### Question 56\n",
    "If you were building a real application, which factors would you consider when choosing between a smaller model (like TinyLlama) vs. a larger model (like Qwen3)?\n",
    "- **Answer:**\n",
    "\n",
    "##### Question 57\n",
    "In what scenarios would you prefer:\n",
    "- **TinyLlama (smaller model):**\n",
    "- **Qwen3 (larger model):**\n",
    "\n",
    "##### Question 58\n",
    "Both models have the same temperature (0.7) and max_new_tokens (256). Why do you think their outputs are still different?\n",
    "- **Answer:**\n",
    "\n",
    "##### Question 59\n",
    "Based on this comparison, what have you learned about the relationship between model size and model capability?\n",
    "- **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 18 - Challenge: Parameter Tuning (Optional)\n",
    "\n",
    "If you have time, experiment with different parameters for Qwen3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.0 - Temperature Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different temperature settings\n",
    "temperatures = [0.1, 0.5, 0.9]\n",
    "test_prompt = \"Write a creative opening sentence for a mystery story.\"\n",
    "\n",
    "print(\"üîß TEMPERATURE EXPERIMENT with Qwen3\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    # Create new pipeline with different temperature\n",
    "    temp_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=qwen_model,\n",
    "        tokenizer=qwen_tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "    )\n",
    "    temp_llm = HuggingFacePipeline(pipeline=temp_pipe)\n",
    "    \n",
    "    print(f\"üå°Ô∏è Temperature: {temp}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = temp_llm.invoke(test_prompt)\n",
    "    print(response)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 60\n",
    "How did changing the temperature affect Qwen3's responses? Which temperature setting do you prefer and why?\n",
    "- **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**üéâ Congratulations on completing the extension activity!**\n",
    "\n",
    "You've now compared two different language models and learned that:\n",
    "- Larger models generally perform better but require more resources\n",
    "- Model size affects accuracy, reasoning, and response quality\n",
    "- There are tradeoffs between speed and capability\n",
    "- The \"best\" model depends on your specific use case\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. **Size matters** - Larger models (more parameters) typically give better results\n",
    "2. **Speed vs. Quality** - Smaller models are faster but less capable\n",
    "3. **Application-specific** - Choose based on whether you need speed or accuracy\n",
    "4. **Resource constraints** - Consider memory, processing power, and cost\n",
    "\n",
    "These insights will help you make informed decisions when building AI applications in the future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
